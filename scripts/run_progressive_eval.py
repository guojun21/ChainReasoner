#!/usr/bin/env python3
"""
Progressive evaluation with early stopping and regression detection.
Stages: 5 -> 20 -> 100 with thresholds 2 and 8.
Outputs JSON array with id + answer only.

== åªè¿›ä¸é€€æœºåˆ¶ ==
è¯„æµ‹å‰è‡ªåŠ¨åŠ è½½å†å²æœ€ä½³ç­”æ¡ˆï¼ˆoutputs/ é‡Œåˆ†æ•°æœ€é«˜çš„æ–‡ä»¶ï¼‰ä½œä¸º baselineã€‚
æ¯å®Œæˆä¸€é“é¢˜ï¼Œå¦‚æœè¯¥é¢˜åœ¨ baseline ä¸­ç­”å¯¹äº†ä½†æœ¬è½®ç­”é”™äº†ï¼Œç«‹å³ç»ˆæ­¢è¯„æµ‹å¹¶æŠ¥å‘Šé€€æ­¥åŸå› ã€‚
è¿™ç¡®ä¿ä»£ç ä¿®æ”¹ä¸ä¼šå¯¼è‡´ä»¥å‰ç­”å¯¹çš„é¢˜çªç„¶ç­”é”™ï¼ˆé€€æ­¥ï¼‰ã€‚

[PERF-OPTIMIZED 2026-02-06] ä½¿ç”¨ ThreadPoolExecutor å¹¶å‘å¤„ç†é¢˜ç›®ï¼Œå¤§å¹…æå‡é€Ÿåº¦ã€‚
  - æ¯é˜¶æ®µå†…çš„é¢˜ç›®å¹¶å‘æ‰§è¡Œï¼ˆé»˜è®¤ 3 ä¸ª workerï¼‰ï¼Œå—é™äº Brave API rate limit
  - æ¯é¢˜å®Œæˆç«‹å³æ‰“å°è€—æ—¶ï¼Œä¾¿äºè§‚å¯Ÿ
  - æ³¨æ„ï¼šå¦‚æœä½ ï¼ˆCursor agentï¼‰æ­£åœ¨ä¿®æ”¹ api_server.py æˆ– constrained_search.pyï¼Œ
    æœ¬è„šæœ¬çš„å¹¶å‘é€»è¾‘ä¸å—å½±å“ï¼Œå› ä¸ºæ¯ä¸ªçº¿ç¨‹å…±äº«åŒä¸€ä¸ª server å®ä¾‹ã€‚

== ç¦æ­¢ä¿®æ”¹è¯„åˆ†é€»è¾‘ ==
_normalize_for_compare() å’Œ score_results() æ˜¯è¯„åˆ†æ ¸å¿ƒå‡½æ•°ï¼Œ
ç¦æ­¢æ·»åŠ ç¼©å†™å½’ä¸€åŒ–ã€åŒ…å«å…³ç³»åŒ¹é…ã€æ¨¡ç³ŠåŒ¹é…ç­‰æ”¾æ¾è¯„åˆ†æ ‡å‡†çš„é€»è¾‘ã€‚
æœ€ç»ˆæ¯”èµ›ç”¨æ¯”èµ›æ–¹çš„è¯„åˆ†ç³»ç»Ÿï¼Œä¸æ˜¯æœ¬åœ°çš„ã€‚
"""

import json
import re
import sys
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path

BASE_DIR = Path(__file__).resolve().parents[1]
AUTORUN_DIR = BASE_DIR.parent / "autorunAtnight"
sys.path.insert(0, str(BASE_DIR))

from apps.api.api_server import EnhancedMultiHopAPIServer

QUESTIONS_FILE = BASE_DIR / "data" / "qa" / "question.json"
STANDARD_FILE = BASE_DIR / "data" / "qa" / "the_standard_answers.json"
OUTPUT_DIR = AUTORUN_DIR / "outputs"
SUMMARY_FILE = OUTPUT_DIR / "last_progressive_summary.json"

# å¹¶å‘ worker æ•°ï¼ˆå— Brave API rate limit çº¦æŸï¼Œä¸å®œè¿‡å¤§ï¼‰
MAX_WORKERS = 3


def _normalize_answer(value):
    if value is None:
        return ""
    return " ".join(str(value).strip().split())


def _normalize_for_compare(value):
    """Normalize answer for comparison: lowercase and strip punctuation.
    ç¦æ­¢ä¿®æ”¹æ­¤å‡½æ•°ï¼ä¸å¾—æ·»åŠ ç¼©å†™å½’ä¸€åŒ–ã€åŒ…å«åŒ¹é…ç­‰æ”¾æ¾è¯„åˆ†çš„é€»è¾‘ã€‚
    """
    if value is None:
        return ""
    text = " ".join(str(value).strip().split()).lower()
    text = text.rstrip(".")
    return text


def load_questions(file_path: Path):
    questions = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                questions.append(json.loads(line))
    return questions


def load_standard_map(file_path: Path):
    with open(file_path, "r", encoding="utf-8") as f:
        standard_data = json.load(f)
    standard_answers = standard_data.get("answers", [])
    return {str(item.get("id")): _normalize_answer(item.get("answer")) for item in standard_answers}


def score_results(results, standard_map):
    """ç¦æ­¢ä¿®æ”¹æ­¤å‡½æ•°ï¼è¯„åˆ†å¿…é¡»ä¿æŒç²¾ç¡®æ–‡æœ¬åŒ¹é…ã€‚"""
    matches = 0
    for item in results:
        qid = str(item.get("id"))
        if qid not in standard_map:
            continue
        our = _normalize_for_compare(item.get("answer"))
        ref = _normalize_for_compare(standard_map[qid])
        if our == ref:
            matches += 1
    return matches


# --------------- åªè¿›ä¸é€€ï¼šå†å²æœ€ä½³ baseline ---------------

def _load_best_baseline(standard_map):
    """æ‰«æ outputs/ ç›®å½•ï¼Œæ‰¾åˆ°åˆ†æ•°æœ€é«˜çš„ç­”æ¡ˆæ–‡ä»¶ï¼Œè¿”å›å…¶ä¸­ç­”å¯¹çš„é¢˜ç›®é›†åˆã€‚
    è¿”å›: (baseline_correct_ids: set, baseline_file: str, baseline_score: int)
    """
    if not OUTPUT_DIR.exists():
        return set(), "", 0

    best_score = -1
    best_file = None

    for f in OUTPUT_DIR.iterdir():
        m = re.match(r"(\d+)åˆ†_\d+_answers\.json$", f.name)
        if m:
            file_score = int(m.group(1))
            if file_score > best_score:
                best_score = file_score
                best_file = f

    if not best_file or best_score <= 0:
        return set(), "", 0

    try:
        with open(best_file, "r", encoding="utf-8") as fh:
            baseline_results = json.load(fh)
    except Exception:
        return set(), "", 0

    correct_ids = set()
    for item in baseline_results:
        qid = str(item.get("id"))
        if qid not in standard_map:
            continue
        our = _normalize_for_compare(item.get("answer"))
        ref = _normalize_for_compare(standard_map[qid])
        if our == ref:
            correct_ids.add(qid)

    return correct_ids, str(best_file), best_score


def _check_regression(results, standard_map, baseline_correct_ids):
    """æ£€æŸ¥æœ¬è½®ç»“æœæ˜¯å¦æœ‰é€€æ­¥ï¼ˆbaseline ç­”å¯¹ä½†æœ¬è½®ç­”é”™çš„é¢˜ï¼‰ã€‚
    è¿”å›: list of {qid, baseline_answer(correct), current_answer, standard_answer}
    """
    regressions = []
    for item in results:
        qid = str(item.get("id"))
        if qid not in baseline_correct_ids:
            continue  # baseline ä¹Ÿæ²¡ç­”å¯¹ï¼Œä¸ç®—é€€æ­¥
        if qid not in standard_map:
            continue
        our = _normalize_for_compare(item.get("answer"))
        ref = _normalize_for_compare(standard_map[qid])
        if our != ref:
            regressions.append({
                "qid": qid,
                "current_answer": item.get("answer"),
                "standard_answer": standard_map[qid],
                "note": f"Q{qid} é€€æ­¥ï¼baseline ç­”å¯¹äº†ï¼Œæœ¬è½®ç­”é”™"
            })
    return regressions


# --------------- /åªè¿›ä¸é€€ ---------------


def save_results(results, score):
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = OUTPUT_DIR / f"{score}åˆ†_{timestamp}_answers.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    return output_file


def write_summary(summary):
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    with open(SUMMARY_FILE, "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    return SUMMARY_FILE


def _answer_one_question(server, q, idx):
    """å¤„ç†å•ä¸ªé¢˜ç›®ï¼Œè¿”å› (idx, {"id": ..., "answer": ...}, elapsed_seconds)ã€‚"""
    qid = q.get("id", idx)
    text = q.get("question", "")
    t0 = time.time()
    try:
        result = server._multi_hop_reasoning(text, use_mcp=True)
        answer = result.get("answer", "Unknown")
    except Exception as e:
        answer = "Unknown"
        print(f"  [ERROR] Q{qid}: {e}", flush=True)
    elapsed = time.time() - t0
    return idx, {"id": qid, "answer": answer}, elapsed


# ç”¨äºæ‰“å°çš„é”ï¼Œé¿å…å¤šçº¿ç¨‹è¾“å‡ºæ··ä¹±
_print_lock = threading.Lock()


def _run_stage_concurrent(server, questions, start_idx, end_idx):
    """å¹¶å‘å¤„ç† [start_idx, end_idx) èŒƒå›´å†…çš„é¢˜ç›®ã€‚è¿”å›æŒ‰åŸå§‹é¡ºåºæ’åˆ—çš„ç»“æœåˆ—è¡¨ã€‚"""
    batch = [(questions[i], i) for i in range(start_idx, end_idx)]
    if not batch:
        return []

    results_dict = {}
    total = len(batch)
    done_count = 0

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {
            executor.submit(_answer_one_question, server, q, idx): idx
            for q, idx in batch
        }
        for future in as_completed(futures):
            idx, result, elapsed = future.result()
            results_dict[idx] = result
            done_count += 1
            qid = result["id"]
            ans_preview = result["answer"][:40]
            with _print_lock:
                print(
                    f"  Q{qid} done ({done_count}/{total}) "
                    f"{elapsed:.1f}s => {ans_preview}",
                    flush=True,
                )

    # æŒ‰åŸå§‹ç´¢å¼•é¡ºåºè¿”å›
    return [results_dict[i] for i in range(start_idx, end_idx)]


def main():
    if not QUESTIONS_FILE.exists():
        print(f"Questions file not found: {QUESTIONS_FILE}")
        return
    if not STANDARD_FILE.exists():
        print(f"Standard answers not found: {STANDARD_FILE}")
        return

    questions = load_questions(QUESTIONS_FILE)
    standard_map = load_standard_map(STANDARD_FILE)
    server = EnhancedMultiHopAPIServer()

    # åŠ è½½å†å²æœ€ä½³ baseline
    baseline_correct_ids, baseline_file, baseline_score = _load_best_baseline(standard_map)
    if baseline_correct_ids:
        print(f"\n[Baseline] å†å²æœ€ä½³: {baseline_score}åˆ† ({baseline_file})", flush=True)
        print(f"[Baseline] ç­”å¯¹çš„é¢˜ç›® ({len(baseline_correct_ids)}é¢˜): {sorted(baseline_correct_ids, key=lambda x: int(x))}", flush=True)
        print(f"[Baseline] æœ¬è½®å¦‚æœè¿™äº›é¢˜ç­”é”™ï¼Œå°†ç«‹å³ç»ˆæ­¢è¯„æµ‹\n", flush=True)
    else:
        print("\n[Baseline] æ— å†å²æœ€ä½³è®°å½•ï¼Œè·³è¿‡é€€æ­¥æ£€æµ‹\n", flush=True)

    stages = [(5, 2), (20, 8), (100, None)]
    results = []
    current_count = 0
    last_output = None

    for stage_total, threshold in stages:
        target = min(stage_total, len(questions))
        if current_count < target:
            stage_t0 = time.time()
            print(f"\n=== Stage {stage_total} ({current_count}â†’{target}) workers={MAX_WORKERS} ===", flush=True)

            stage_results = _run_stage_concurrent(server, questions, current_count, target)
            results.extend(stage_results)

            stage_elapsed = time.time() - stage_t0
            print(f"=== Stage {stage_total} done in {stage_elapsed:.1f}s ===", flush=True)

        current_count = target
        score = score_results(results, standard_map)

        # ---- åªè¿›ä¸é€€ï¼šæ£€æŸ¥é€€æ­¥ ----
        if baseline_correct_ids:
            regressions = _check_regression(results, standard_map, baseline_correct_ids)
            if regressions:
                # è¿˜æ˜¯å…ˆä¿å­˜ç»“æœï¼Œæ–¹ä¾¿åˆ†æ
                last_output = save_results(results, score)

                print(f"\n{'='*60}", flush=True)
                print(f"ğŸš¨ é€€æ­¥æ£€æµ‹å¤±è´¥ï¼æœ¬è½®å¾— {score} åˆ†ï¼Œä½†æœ‰ {len(regressions)} é“é¢˜é€€æ­¥äº†", flush=True)
                print(f"   Baseline: {baseline_score}åˆ† ({baseline_file})", flush=True)
                print(f"{'='*60}", flush=True)
                for reg in regressions:
                    print(f"  âŒ Q{reg['qid']}: baseline ç­”å¯¹ \"{reg['standard_answer']}\"", flush=True)
                    print(f"     æœ¬è½®ç­”äº† \"{reg['current_answer']}\" â€” é€€æ­¥ï¼", flush=True)
                print(f"{'='*60}", flush=True)
                print(f"\nç»“è®ºï¼šä»£ç ä¿®æ”¹å¯¼è‡´é€€æ­¥ï¼Œå¿…é¡»å›æ»šæˆ–ä¿®å¤ä»¥ä¸Šé€€æ­¥çš„é¢˜ç›®ã€‚", flush=True)
                print(f"æœ¬è½®æ–°ç­”å¯¹ä½† baseline æ²¡å¯¹çš„é¢˜ä¸èƒ½ä»¥ç‰ºç‰²å·²å¯¹çš„é¢˜ä¸ºä»£ä»·ã€‚\n", flush=True)

                # å†™ summary æ ‡è®°å¤±è´¥å’Œé€€æ­¥åŸå› 
                regression_detail = [
                    f"Q{r['qid']}: baselineæ­£ç¡®=\"{r['standard_answer']}\" æœ¬è½®é”™è¯¯=\"{r['current_answer']}\""
                    for r in regressions
                ]
                summary = {
                    "stage_total": stage_total,
                    "ran": current_count,
                    "score": score,
                    "threshold": threshold,
                    "passed": False,
                    "output_file": str(last_output),
                    "regression_detected": True,
                    "regression_count": len(regressions),
                    "regression_detail": regression_detail,
                    "baseline_score": baseline_score,
                    "baseline_file": baseline_file,
                    "failure_reason": f"é€€æ­¥{len(regressions)}é¢˜: " + "; ".join(
                        f"Q{r['qid']}(\"{r['standard_answer']}\"â†’\"{r['current_answer']}\")"
                        for r in regressions
                    )
                }
                write_summary(summary)
                print(f"Output: {last_output}")
                print(f"Summary: {SUMMARY_FILE}")
                return  # ç«‹å³ç»ˆæ­¢
        # ---- /åªè¿›ä¸é€€ ----

        last_output = save_results(results, score)

        summary = {
            "stage_total": stage_total,
            "ran": current_count,
            "score": score,
            "threshold": threshold,
            "passed": True if threshold is None else score >= threshold,
            "output_file": str(last_output)
        }
        write_summary(summary)
        print(f"Stage {stage_total} Score: {score}")
        print(f"Output: {last_output}")
        print(f"Summary: {SUMMARY_FILE}")

        if threshold is not None and score < threshold:
            print("Stage failed, stopping early.")
            break


if __name__ == "__main__":
    main()
